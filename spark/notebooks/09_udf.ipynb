{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/logo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "\n",
    "## Podstawy Spark SQL - UDF\n",
    "\n",
    "Podobnie jak w Hive czy wielu bazach danych, Spark SQL ma możliwość definiowania funkcji użytkownika, ang. User Defined Functions (UDF). Funkcje te biorą wartość z kolumny i przekształcają ją w inną wartość. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as types\n",
    "sc = pyspark.SparkContext(appName='udf')\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy najpierw dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = sqlContext.createDataFrame([pyspark.sql.Row(kolumna=int(i)) for i in np.random.randint(0, 100, 100)])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do stworzenia funkcji używamy normalnej referencji do funkcji Python lub lambdy. Przykładowo, chcemy funkcję która zwróci klasyfikacje wartości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klasyfikuj(wartosc):\n",
    "    return u'dużo' if wartosc > 50 else u'mało'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższą funkcję należy teraz przekształcić w funkcje która działa na kolumnach. Robimy to używając funkcji `udf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klasyfikuj_udf = func.udf(klasyfikuj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tej funkcji można już użyć na kolumnie DataFrame, np możemy dodać kolumnę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('ile?', klasyfikuj_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należu uważać z typami; jeżeli chcemy zwrócić inny typ niż tekstowy, należy przekazać tą informację w definicji funkcji UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oblicz_udf = func.udf(lambda v: v + 123, returnType=types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('oblicz', oblicz_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wykorzystać funckje w zapytaniach SQL należy ją zarejestrować nieco inaczej, mianowicie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.udf.register('klasyfikuj', klasyfikuj, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('df')\n",
    "sqlContext.sql('SELECT kolumna, klasyfikuj(kolumna) AS `ile?` FROM df').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do JVM\n",
    "\n",
    "Spark używa [py4j](https://www.py4j.org/) aby wykonywać komendy na JVM. Dostęp do klas mamy w PySpark nieco ułatwiony, mianowicie używamy `_jvm` ze `SparkContext` podając pełną nazwę klasy z pakietem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sc._jvm.java.lang.String('tekst')\n",
    "print(s, type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, py4j konwertuje typy. Możemy też użyć bardziej skomplikowanych obiektów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = sc._jvm.java.util.StringTokenizer('Ala ma kota!')\n",
    "print(tokenizer, type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(tokenizer.hasMoreTokens()):\n",
    "    print(tokenizer.nextToken())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lub prościej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sc._jvm.java.util.regex.Pattern.compile('\\\\s')\n",
    "print(p, type(p))\n",
    "l = p.split('Ala ma kota!')\n",
    "print(l, type(l))\n",
    "list(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście lepiej powyższą funkcjonalność wykorzystać do funkcji zwracających RDD lub DataFrame, które można użyć bezpośrednio w Pythonie."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
