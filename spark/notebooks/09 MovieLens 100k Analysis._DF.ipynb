{"cells":[{"cell_type":"markdown","source":["## Publicly available datasets\n\nUCI Machine Learning Repository: This is a collection of almost 300 datasets of various types and sizes for tasks including classification, regression, clustering, and recommender systems. The list is available at http://archive.ics.uci.edu/ml/.\n\nAmazon AWS public datasets: This is a set of often very large datasets that can be accessed via Amazon S3. These datasets include the Human Genome Project, the Common Crawl web corpus, Wikipedia data, and Google Books Ngrams. Information on these datasets can be found at http://aws.amazon.com/publicdatasets/.\n\nKaggle: This is a collection of datasets used in machine learning competitions run by Kaggle. Areas include classification, regression, ranking, recommender systems, and image analysis. These datasets can be found under the Competitions section at http://www.kaggle.com/competitions.\n\nKDnuggets: This has a detailed list of public datasets, including some of those mentioned earlier. The list is available at http://www.kdnuggets.com/datasets/index.html."],"metadata":{}},{"cell_type":"code","source":["#%pylab inline\n#%matplotlib notebook\n#import matplotlib.pyplot as plt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">%matplotlib notebook is not supported in Databricks.\nYou can display matplotlib figures using display(). For an example, see https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## Exploring the User Dataset"],"metadata":{}},{"cell_type":"code","source":["# from pyspark import SparkContext\n# sc = SparkContext(\"local[*]\", \"MovieLens\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-558673545335494&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">from</span> pyspark <span class=\"ansigreen\">import</span> SparkContext<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>sc <span class=\"ansiyellow\">=</span> SparkContext<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;local[*]&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;MovieLens&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">__init__</span><span class=\"ansiblue\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n<span class=\"ansigreen\">    131</span>                     &quot; note this option will be removed in Spark 3.0&quot;)\n<span class=\"ansigreen\">    132</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 133</span><span class=\"ansiyellow\">         </span>SparkContext<span class=\"ansiyellow\">.</span>_ensure_initialized<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> gateway<span class=\"ansiyellow\">=</span>gateway<span class=\"ansiyellow\">,</span> conf<span class=\"ansiyellow\">=</span>conf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    134</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    135</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">_ensure_initialized</span><span class=\"ansiblue\">(cls, instance, gateway, conf)</span>\n<span class=\"ansigreen\">    335</span>                         <span class=\"ansiblue\">&quot; created by %s at %s:%s &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    336</span>                         % (currentAppName, currentMaster,\n<span class=\"ansigreen\">--&gt; 337</span><span class=\"ansiyellow\">                             callsite.function, callsite.file, callsite.linenum))\n</span><span class=\"ansigreen\">    338</span>                 <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    339</span>                     SparkContext<span class=\"ansiyellow\">.</span>_active_spark_context <span class=\"ansiyellow\">=</span> instance<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /local_disk0/tmp/1569430980881-0/PythonShell.py:1133 </div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark import Row\nuser_data = sc.textFile(\"/FileStore/tables/u.user\")\nuser_data.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>&apos;1|24|M|technician|85711&apos;</div>"]}}],"execution_count":5},{"cell_type":"code","source":["def to_bool(value):\n    '''\n    Converts values (0, 1 (non-zero)) to boolean\n    \n    @param value: int value to convert\n    '''\n    v = int(value)\n    return False if v == 0 else True"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def data_from_csv(line):\n    '''\n    Converts a line of data table from CSV to DataFrame Row\n    \n    @param line: line of data row \n    @returns: Row of parsed values\n    '''\n    c = line.split('\\t')\n    \n    row = dict()\n    row['userId'] = int(c[0])\n    row['itemId'] = int(c[1])\n    row['rating'] = int(c[2])\n    row['timestamp'] = int(c[3]) # Timestamp Unix to long ale w Python 3 int to zar√≥wno int jak long z Python 2.\n    \n    return Row(**row)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["def user_from_csv(line):\n    '''\n    Converts a line of user table from CSV to DataFrame Row\n    \n    @param line: line of user row \n    @returns: Row of parsed values\n    '''\n    c = line.split('|')\n    \n    row = dict()\n    row['userId'] = int(c[0])\n    row['age'] = str(c[1])\n    row['gender'] = str(c[2])\n    row['occupation'] = str(c[3])\n    row['zipCode'] = str(c[4])\n        \n    return Row(**row)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def item_from_csv(line):\n    '''\n    Converts a line of item table from CSV to DataFrame Row\n    \n    @param line: line of item row \n    @returns: Row of parsed values\n    '''\n    c = line.split('|')\n    \n    row = dict()\n    row['movieId'] = int(c[0])\n    row['movieTitle'] = str(c[1])\n    row['releaseDate'] = str(c[2])\n    row['videoReleaseDate'] = str(c[3])\n    row['imdbUrl'] = str(c[4])\n    row['unknown'] = to_bool(c[5])\n    row['action'] = to_bool(c[6])\n    row['adventure'] = to_bool(c[7])\n    row['animation'] = to_bool(c[8])\n    row['childrens'] = to_bool(c[9])\n    row['comedy'] = to_bool(c[10])\n    row['crime'] = to_bool(c[11])\n    row['documentary'] = to_bool(c[12])\n    row['drama'] = to_bool(c[13])\n    row['fantasy'] = to_bool(c[14])\n    row['filmNoir'] = to_bool(c[15])\n    row['horror'] = to_bool(c[16])\n    row['musical'] = to_bool(c[17])\n    row['mystery'] = to_bool(c[18])\n    row['romance'] = to_bool(c[19])\n    row['sciFi'] = to_bool(c[20])\n    row['thriller'] = to_bool(c[21])\n    row['war'] = to_bool(c[22])\n    row['western'] = to_bool(c[23])\n    \n    return Row(**row)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["user_data_rdd = user_data.map(user_from_csv)\nuser_data_df = sqlContext.createDataFrame(user_data_rdd)\nuser_data_df.printSchema()\nuser_data_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- age: string (nullable = true)\n-- gender: string (nullable = true)\n-- occupation: string (nullable = true)\n-- userId: long (nullable = true)\n-- zipCode: string (nullable = true)\n\n+---+------+-------------+------+-------+\nage|gender|   occupation|userId|zipCode|\n+---+------+-------------+------+-------+\n 24|     M|   technician|     1|  85711|\n 53|     F|        other|     2|  94043|\n 23|     M|       writer|     3|  32067|\n 24|     M|   technician|     4|  43537|\n 33|     F|        other|     5|  15213|\n 42|     M|    executive|     6|  98101|\n 57|     M|administrator|     7|  91344|\n 36|     M|administrator|     8|  05201|\n 29|     M|      student|     9|  01002|\n 53|     M|       lawyer|    10|  90703|\n 39|     F|        other|    11|  30329|\n 28|     F|        other|    12|  06405|\n 47|     M|     educator|    13|  29206|\n 45|     M|    scientist|    14|  55106|\n 49|     F|     educator|    15|  97301|\n 21|     M|entertainment|    16|  10309|\n 30|     M|   programmer|    17|  06355|\n 35|     F|        other|    18|  37212|\n 40|     M|    librarian|    19|  02138|\n 42|     F|    homemaker|    20|  95660|\n+---+------+-------------+------+-------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["user_fields = user_data.map(lambda line: line.split(\"|\"))\nnum_users = user_fields.map(lambda fields: fields[0]).count()\nnum_genders = user_fields.map(lambda fields: fields[2]).distinct().count()\nnum_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\nnum_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()\nprint (\"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Users: 943, genders: 2, occupations: 21, ZIP codes: 795\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# ages = user_data_df.select('age')(#user_fields.map(lambda x: int(x[1]))#.collect()\ndisplay(user_data_df.groupBy(\"age\").count().orderBy(\"age\"))"],"metadata":{"scrolled":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>count</th></tr></thead><tbody><tr><td>10</td><td>1</td></tr><tr><td>11</td><td>1</td></tr><tr><td>13</td><td>5</td></tr><tr><td>14</td><td>3</td></tr><tr><td>15</td><td>6</td></tr><tr><td>16</td><td>5</td></tr><tr><td>17</td><td>14</td></tr><tr><td>18</td><td>18</td></tr><tr><td>19</td><td>23</td></tr><tr><td>20</td><td>32</td></tr><tr><td>21</td><td>27</td></tr><tr><td>22</td><td>37</td></tr><tr><td>23</td><td>28</td></tr><tr><td>24</td><td>33</td></tr><tr><td>25</td><td>38</td></tr><tr><td>26</td><td>34</td></tr><tr><td>27</td><td>35</td></tr><tr><td>28</td><td>36</td></tr><tr><td>29</td><td>32</td></tr><tr><td>30</td><td>39</td></tr><tr><td>31</td><td>25</td></tr><tr><td>32</td><td>28</td></tr><tr><td>33</td><td>26</td></tr><tr><td>34</td><td>17</td></tr><tr><td>35</td><td>27</td></tr><tr><td>36</td><td>21</td></tr><tr><td>37</td><td>19</td></tr><tr><td>38</td><td>17</td></tr><tr><td>39</td><td>22</td></tr><tr><td>40</td><td>21</td></tr><tr><td>41</td><td>10</td></tr><tr><td>42</td><td>21</td></tr><tr><td>43</td><td>13</td></tr><tr><td>44</td><td>23</td></tr><tr><td>45</td><td>15</td></tr><tr><td>46</td><td>12</td></tr><tr><td>47</td><td>14</td></tr><tr><td>48</td><td>20</td></tr><tr><td>49</td><td>19</td></tr><tr><td>50</td><td>20</td></tr><tr><td>51</td><td>20</td></tr><tr><td>52</td><td>6</td></tr><tr><td>53</td><td>12</td></tr><tr><td>54</td><td>4</td></tr><tr><td>55</td><td>11</td></tr><tr><td>56</td><td>6</td></tr><tr><td>57</td><td>9</td></tr><tr><td>58</td><td>3</td></tr><tr><td>59</td><td>3</td></tr><tr><td>60</td><td>9</td></tr><tr><td>61</td><td>3</td></tr><tr><td>62</td><td>2</td></tr><tr><td>63</td><td>3</td></tr><tr><td>64</td><td>2</td></tr><tr><td>65</td><td>3</td></tr><tr><td>66</td><td>1</td></tr><tr><td>68</td><td>2</td></tr><tr><td>69</td><td>2</td></tr><tr><td>7</td><td>1</td></tr><tr><td>70</td><td>3</td></tr><tr><td>73</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["import numpy as np\ncount_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\nx_axis1 = np.array([c[0] for c in count_by_occupation])\ny_axis1 = np.array([c[1] for c in count_by_occupation])\nx_axis = x_axis1[np.argsort(y_axis1)]\ny_axis = y_axis1[np.argsort(y_axis1)]\n\npos = np.arange(len(x_axis))\nwidth = 1.0\n\nax = plt.axes()\nax.set_xticks(pos + (width / 2))\nax.set_xticklabels(x_axis)\n\nplt.bar(pos, y_axis, width, color='lightblue')\nplt.xticks(rotation=30)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-558673545335498&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      9</span> width <span class=\"ansiyellow\">=</span> <span class=\"ansicyan\">1.0</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 11</span><span class=\"ansiyellow\"> </span>ax <span class=\"ansiyellow\">=</span> plt<span class=\"ansiyellow\">.</span>axes<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span> ax<span class=\"ansiyellow\">.</span>set_xticks<span class=\"ansiyellow\">(</span>pos <span class=\"ansiyellow\">+</span> <span class=\"ansiyellow\">(</span>width <span class=\"ansiyellow\">/</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> ax<span class=\"ansiyellow\">.</span>set_xticklabels<span class=\"ansiyellow\">(</span>x_axis<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;plt&apos; is not defined</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Note we can also use the Spark RDD method 'countByValue' to generate the occupation counts\ncount_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\nprint (\"Map-reduce approach:\")\nprint (dict(count_by_occupation2))\nprint (\"\")\nprint (\"countByValue approach:\")\nprint (dict(count_by_occupation))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Exploring the Movie Dataset"],"metadata":{}},{"cell_type":"code","source":["movie_data = sc.textFile(\"%s/ml-100k/u.item\" % PATH)\nprint (movie_data.first())\nnum_movies = movie_data.count()\nprint (\"Movies: %d\" % num_movies)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def convert_year(x):\n    try:\n        return int(x[-4:])\n    except:\n        return 1900 # there is a 'bad' data point with a blank year, which we set to 1900 and will filter out later\n\nmovie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\nyears = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n# we filter out any 'bad' data points here\nyears_filtered = years.filter(lambda x: x != 1900)\n# plot the movie ages histogram\nmovie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\nvalues = list(movie_ages.values())#\nbins = list(movie_ages)#"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["plt.hist(values, bins=bins, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(17,10)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Exploring the Rating Dataset"],"metadata":{}},{"cell_type":"code","source":["rating_data_raw = sc.textFile(\"%s/ml-100k/u.data\" % PATH)\nprint (rating_data_raw.first())\nnum_ratings = rating_data_raw.count()\nprint (\"Ratings: %d\" % num_ratings)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["rating_data = rating_data_raw.map(lambda line: line.split(\"\\t\"))\nratings = rating_data.map(lambda fields: int(fields[2]))\nmax_rating = ratings.reduce(lambda x, y: max(x, y))\nmin_rating = ratings.reduce(lambda x, y: min(x, y))\nmean_rating = ratings.reduce(lambda x, y: x + y) / float(num_ratings)\nmedian_rating = np.median(ratings.collect())\nratings_per_user = num_ratings / num_users\nratings_per_movie = num_ratings / num_movies\nprint (\"Min rating: %d\" % min_rating)\nprint (\"Max rating: %d\" % max_rating)\nprint (\"Average rating: %2.2f\" % mean_rating)\nprint (\"Median rating: %d\" % median_rating)\nprint (\"Average # of ratings per user: %2.2f\" % ratings_per_user)\nprint (\"Average # of ratings per movie: %2.2f\" % ratings_per_movie)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# we can also use the stats function to get some similar information to the above\nratings.stats()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# create plot of counts by rating value\ncount_by_rating = ratings.countByValue()\nx_axis = np.array(list(count_by_rating.keys()))\ny_axis = np.array([float(c) for c in count_by_rating.values()])\n# we normalize the y-axis here to percentages\ny_axis_normed = y_axis / y_axis.sum()\n\npos = np.arange(len(list(x_axis)))\nwidth = 1.0\n\nax = plt.axes()\nax.set_xticks(pos + (width / 2))\nax.set_xticklabels(x_axis)\n\nplt.bar(pos, y_axis_normed, width, color='lightblue')\nplt.xticks(rotation=30)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16, 10)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["list(count_by_rating.values())"],"metadata":{"scrolled":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":["count_by_rating"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["list(count_by_rating)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# to compute the distribution of ratings per user, we first group the ratings by user id\nuser_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))). \\\n    groupByKey() \n# then, for each key (user id), we find the size of the set of ratings, which gives us the # ratings for that user \nuser_ratings_byuser = user_ratings_grouped.map(lambda kv: (kv[0], len(kv[1])))#map(lambda (k, v): (k, len(v)))\nuser_ratings_byuser.take(5)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# and finally plot the histogram\nuser_ratings_byuser_local = user_ratings_byuser.map(lambda kv: kv[1]).collect()\nhist(user_ratings_byuser_local, bins=200, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(16,10)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Filling in Bad or Missing Values"],"metadata":{}},{"cell_type":"code","source":["years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).filter(lambda yr: yr != 1900).collect()\nyears_pre_processed_arr = np.array(years_pre_processed)   \n# first we compute the mean and median year of release, without the 'bad' data point\nmean_year = np.mean(years_pre_processed_arr[years_pre_processed_arr!=1900])\nmedian_year = np.median(years_pre_processed_arr[years_pre_processed_arr!=1900])\nidx_bad_data = np.where(years_pre_processed_arr==1900)[0]#[0]\nyears_pre_processed_arr[idx_bad_data] = median_year\nprint (\"Mean year of release: %d\" % mean_year)\nprint (\"Median year of release: %d\" % median_year)\nprint (\"Index of '1900' after assigning median: %s\" % np.where(years_pre_processed_arr == 1900)[0])"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["##Feature Extraction"],"metadata":{}},{"cell_type":"markdown","source":["### Categorical Features: _1-of-k_ Encoding of User Occupation"],"metadata":{}},{"cell_type":"code","source":["all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\nall_occupations.sort()\n# create a new dictionary to hold the occupations, and assign the \"1-of-k\" indexes\nidx = 0\nall_occupations_dict = {}\nfor o in all_occupations:\n    all_occupations_dict[o] = idx\n    idx +=1\n# try a few examples to see what \"1-of-k\" encoding is assigned\nprint (\"Encoding of 'doctor': %d\" % all_occupations_dict['doctor'])\nprint (\"Encoding of 'programmer': %d\" % all_occupations_dict['programmer'])"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# create a vector representation for \"programmer\" and encode it into a binary vector\nK = len(all_occupations_dict)\nbinary_x = np.zeros(K)\nk_programmer = all_occupations_dict['programmer']\nbinary_x[k_programmer] = 1\nprint (\"Binary feature vector: %s\" % binary_x)\nprint (\"Length of binary vector: %d\" % K)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Transforming Timestamps into Categorical Features"],"metadata":{}},{"cell_type":"code","source":["# a function to extract the timestamps (in seconds) from the dataset\ndef extract_datetime(ts):\n    import datetime\n    return datetime.datetime.fromtimestamp(ts)\n    \ntimestamps = rating_data.map(lambda fields: int(fields[3]))\nhour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\nhour_of_day.take(5)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# a function for assigning \"time-of-day\" bucket given an hour of the day\ndef assign_tod(hr):\n    times_of_day = {\n                'morning' : range(7, 12),\n                'lunch' : range(12, 14),\n                'afternoon' : range(14, 18),\n                'evening' : range(18, 23),\n                'night' : range(23, 7)\n                }\n    for k, v in times_of_day.items():\n        if hr in v: \n            return k\n\n# now apply the \"time of day\" function to the \"hour of day\" RDD\ntime_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\ntime_of_day.take(5)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Simple Text Feature Extraction"],"metadata":{}},{"cell_type":"code","source":["# we define a function to extract just the title from the raw movie title, removing the year of release\ndef extract_title(raw):\n    import re\n    grps = re.search(\"\\((\\w+)\\)\", raw)    # this regular expression finds the non-word (numbers) between parentheses\n    if grps:\n        return raw[:grps.start()].strip() # we strip the trailing whitespace from the title\n    else:\n        return raw\n\n# first lets extract the raw movie titles from the movie fields\nraw_titles = movie_fields.map(lambda fields: fields[1])\n# next, we strip away the \"year of release\" to leave us with just the title text\n# let's test our title extraction function on the first 5 titles\nfor raw_title in raw_titles.take(5):\n    print (extract_title(raw_title))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# ok that looks good! let's apply it to all the titles\nmovie_titles = raw_titles.map(lambda m: extract_title(m))\n# next we tokenize the titles into terms. We'll use simple whitespace tokenization\ntitle_terms = movie_titles.map(lambda t: t.split(\" \"))\nprint (title_terms.take(5))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# next we would like to collect all the possible terms, in order to build out dictionary of term <-> index mappings\nall_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n# create a new dictionary to hold the terms, and assign the \"1-of-k\" indexes\nidx = 0\nall_terms_dict = {}\nfor term in all_terms:\n    all_terms_dict[term] = idx\n    idx +=1\nnum_terms = len(all_terms_dict)\nprint (\"Total number of terms: %d\" % num_terms)\nprint (\"Index of term 'Dead': %d\" % all_terms_dict['Dead'])\nprint (\"Index of term 'Rooms': %d\" % all_terms_dict['Rooms'])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# we could also use Spark's 'zipWithIndex' RDD function to create the term dictionary\nall_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\nprint (\"Index of term 'Dead': %d\" % all_terms_dict2['Dead'])\nprint (\"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms'])"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# this function takes a list of terms and encodes it as a scipy sparse vector using an approach \n# similar to the 1-of-k encoding\ndef create_vector(terms, term_dict):\n    from scipy import sparse as sp\n    x = sp.csc_matrix((1, num_terms))\n    for t in terms:\n        if t in term_dict:\n            idx = term_dict[t]\n            x[0, idx] = 1\n    return x\nall_terms_bcast = sc.broadcast(all_terms_dict)\nterm_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\nterm_vectors.take(5)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Normalizing Features"],"metadata":{}},{"cell_type":"markdown","source":["### Scaling the Norm of Vectors"],"metadata":{}},{"cell_type":"code","source":["np.random.seed(42)\nx = np.random.randn(10)\nnorm_x_2 = np.linalg.norm(x)\nnormalized_x = x / norm_x_2\nprint (\"x:\\n%s\" % x)\nprint (\"2-Norm of x: %2.4f\" % norm_x_2)\nprint (\"Normalized x:\\n%s\" % normalized_x)\nprint (\"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### Scaling the Norm of Vectors with MLlib's Normalizer"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.feature import Normalizer\nnormalizer = Normalizer()\nvector = sc.parallelize([x])\nnormalized_x_mllib = normalizer.transform(vector).first().toArray()\n\nprint (\"x:\\n%s\" % x)\nprint (\"2-Norm of x: %2.4f\" % norm_x_2)\nprint (\"Normalized x MLlib:\\n%s\" % normalized_x_mllib)\nprint (\"2-Norm of normalized_x_mllib: %2.4f\" % np.linalg.norm(normalized_x_mllib))"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.4","nbconvert_exporter":"python","file_extension":".py"},"name":"09 MovieLens 100k Analysis._DF","notebookId":558673545335490,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
