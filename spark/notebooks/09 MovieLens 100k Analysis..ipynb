{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publicly available datasets\n",
    "\n",
    "UCI Machine Learning Repository: This is a collection of almost 300 datasets of various types and sizes for tasks including classification, regression, clustering, and recommender systems. The list is available at http://archive.ics.uci.edu/ml/.\n",
    "\n",
    "Amazon AWS public datasets: This is a set of often very large datasets that can be accessed via Amazon S3. These datasets include the Human Genome Project, the Common Crawl web corpus, Wikipedia data, and Google Books Ngrams. Information on these datasets can be found at http://aws.amazon.com/publicdatasets/.\n",
    "\n",
    "Kaggle: This is a collection of datasets used in machine learning competitions run by Kaggle. Areas include classification, regression, ranking, recommender systems, and image analysis. These datasets can be found under the Competitions section at http://www.kaggle.com/competitions.\n",
    "\n",
    "KDnuggets: This has a detailed list of public datasets, including some of those mentioned earlier. The list is available at http://www.kdnuggets.com/datasets/index.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# %matplotlib notebook\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the User Dataset\n",
    "\n",
    "You can download the dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"MovieLens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will analyze the characteristics of MovieLens users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this PATH with the correct path to the MovieLens dataset on your computer\n",
    "PATH = \"/home/rafalpa/BIGDATA/examples/spark/notebooks/data\"\n",
    "user_data = sc.textFile(\"%s/ml-100k/u.user\" % PATH)\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the data by splitting each line, around the \"|\" character. This will give us an RDD where each record is a Python list that contains the user ID, age, gender, occupation, and ZIP code fields.\n",
    "\n",
    "We will then count the number of users, genders, occupations, and ZIP codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
    "num_users = user_fields.map(lambda fields: fields[0]).count()\n",
    "num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()\n",
    "num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\n",
    "num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()\n",
    "print (\"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ages = user_fields.map(lambda x: int(x[1])).collect()\n",
    "hist(ages, bins=20, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to explore the relative frequencies of the various occupations of our users. We can do this using the following code snippet. First, we will use the MapReduce approach introduced previously to count the occurrences of each occupation in the dataset. Then, we will use matplotlib to display a bar chart of occupation counts, using the bar function.\n",
    "\n",
    "Since part of our data is the descriptions of textual occupation, we will need to manipulate it a little to get it to work with the bar function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "x_axis1 = np.array([c[0] for c in count_by_occupation])\n",
    "y_axis1 = np.array([c[1] for c in count_by_occupation])\n",
    "x_axis = x_axis1[np.argsort(y_axis1)]\n",
    "y_axis = y_axis1[np.argsort(y_axis1)]\n",
    "\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis, width, color='lightblue')\n",
    "plt.xticks(rotation=30)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we can also use the Spark RDD method 'countByValue' to generate the occupation counts\n",
    "count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\n",
    "print (\"Map-reduce approach:\")\n",
    "print (dict(count_by_occupation2))\n",
    "print (\"\")\n",
    "print (\"countByValue approach:\")\n",
    "print (dict(count_by_occupation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Movie Dataset\n",
    "\n",
    "Next, we will investigate a few properties of the movie catalogue. We can inspect a row of the movie data file, as we did for the user data earlier, and then count the number of movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = sc.textFile(\"%s/ml-100k/u.item\" % PATH)\n",
    "print (movie_data.first())\n",
    "num_movies = movie_data.count()\n",
    "print (\"Movies: %d\" % num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_year(x):\n",
    "    try:\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1900 # there is a 'bad' data point with a blank year, which we set to 1900 and will filter out later\n",
    "\n",
    "movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\n",
    "years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n",
    "# we filter out any 'bad' data points here\n",
    "years_filtered = years.filter(lambda x: x != 1900)\n",
    "# plot the movie ages histogram\n",
    "movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\n",
    "values = list(movie_ages.values())\n",
    "bins = list(movie_ages)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example of how real-world datasets can often be messy and require a more in-depth approach to parsing data. In fact, this also illustrates why data exploration is so important, as many of these issues in data integrity and quality are picked up during this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(values, bins=bins, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(17,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Rating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data_raw = sc.textFile(\"%s/ml-100k/u.data\" % PATH)\n",
    "print (rating_data_raw.first())\n",
    "num_ratings = rating_data_raw.count()\n",
    "print (\"Ratings: %d\" % num_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data = rating_data_raw.map(lambda line: line.split(\"\\t\"))\n",
    "ratings = rating_data.map(lambda fields: int(fields[2]))\n",
    "max_rating = ratings.reduce(lambda x, y: max(x, y))\n",
    "min_rating = ratings.reduce(lambda x, y: min(x, y))\n",
    "mean_rating = ratings.reduce(lambda x, y: x + y) / float(num_ratings)\n",
    "median_rating = np.median(ratings.collect())\n",
    "ratings_per_user = num_ratings / num_users\n",
    "ratings_per_movie = num_ratings / num_movies\n",
    "print (\"Min rating: %d\" % min_rating)\n",
    "print (\"Max rating: %d\" % max_rating)\n",
    "print (\"Average rating: %2.2f\" % mean_rating)\n",
    "print (\"Median rating: %d\" % median_rating)\n",
    "print (\"Average # of ratings per user: %2.2f\" % ratings_per_user)\n",
    "print (\"Average # of ratings per movie: %2.2f\" % ratings_per_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the stats function to get some similar information to the above\n",
    "ratings.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, the average rating given by a user to a movie is around 3.5 and the median rating is 4, so we might expect that the distribution of ratings will be skewed towards slightly higher ratings. Let's see whether this is true by creating a bar chart of rating values using a similar procedure as we did for occupations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of counts by rating value\n",
    "count_by_rating = ratings.countByValue()\n",
    "x_axis = np.array(list(count_by_rating.keys()))\n",
    "y_axis = np.array([float(c) for c in count_by_rating.values()])\n",
    "# we normalize the y-axis here to percentages\n",
    "y_axis_normed = y_axis / y_axis.sum()\n",
    "\n",
    "pos = np.arange(len(list(x_axis)))\n",
    "width = 1.0\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos, y_axis_normed, width, color='lightblue')\n",
    "plt.xticks(rotation=30)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(count_by_rating.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(count_by_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute the distribution of ratings per user, we first group the ratings by user id\n",
    "user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))). \\\n",
    "    groupByKey() \n",
    "# then, for each key (user id), we find the size of the set of ratings, which gives us the # ratings for that user \n",
    "user_ratings_byuser = user_ratings_grouped.map(lambda kv: (kv[0], len(kv[1])))#map(lambda (k, v): (k, len(v)))\n",
    "user_ratings_byuser.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the users give fewer than 100 ratings. The distribution of the ratings shows, however, that there are fairly large number of users that provide hundreds of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally plot the histogram\n",
    "user_ratings_byuser_local = user_ratings_byuser.map(lambda kv: kv[1]).collect()\n",
    "hist(user_ratings_byuser_local, bins=200, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Bad or Missing Values\n",
    "\n",
    "Missing values and outliers are also common and can be dealt with in a manner similar to bad data. Overall, the broad options are as follows:\n",
    "\n",
    "__Filter out or remove records with bad or missing values__: This is sometimes unavoidable; however, this means losing the good part of a bad or missing record.\n",
    "\n",
    "__Fill in bad or missing data:__ We can try to assign a value to bad or missing data based on the rest of the data we have available. Approaches can include assigning a zero value, assigning the global mean or median, interpolating nearby or similar data points (usually, in a time-series dataset), and so on. Deciding on the correct approach is often a tricky task and depends on the data, situation, and one's own experience.\n",
    "\n",
    "__Apply robust techniques to outliers:__ The main issue with outliers is that they might be correct values, even though they are extreme. They might also be errors. It is often very difficult to know which case you are dealing with. Outliers can also be removed or filled in, although fortunately, there are statistical techniques (such as robust regression) to handle outliers and extreme values.\n",
    "\n",
    "__Apply transformations to potential outliers:__ Another approach for outliers or extreme values is to apply transformations, such as a logarithmic or Gaussian kernel transformation, to features that have potential outliers, or display large ranges of potential values. These types of transformations have the effect of dampening the impact of large changes in the scale of a variable and turning a nonlinear relationship into one that is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_pre_processed = movie_fields.map(lambda fields: fields[2]) \\\n",
    "    .map(lambda x: convert_year(x)) \\\n",
    "    .filter(lambda yr: yr != 1900) \\\n",
    "    .collect()\n",
    "years_pre_processed_arr = np.array(years_pre_processed)   \n",
    "# first we compute the mean and median year of release, without the 'bad' data point\n",
    "mean_year = np.mean(years_pre_processed_arr[years_pre_processed_arr!=1900])\n",
    "median_year = np.median(years_pre_processed_arr[years_pre_processed_arr!=1900])\n",
    "idx_bad_data = np.where(years_pre_processed_arr==1900)[0]#[0]\n",
    "years_pre_processed_arr[idx_bad_data] = median_year\n",
    "print (\"Mean year of release: %d\" % mean_year)\n",
    "print (\"Median year of release: %d\" % median_year)\n",
    "print (\"Index of '1900' after assigning median: %s\" % np.where(years_pre_processed_arr == 1900)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features: _1-of-k_ Encoding of User Occupation\n",
    "\n",
    "Once we have completed the initial exploration, processing, and cleaning of our data, we are ready to get down to the business of extracting actual features from the data, with which our machine learning model can be trained.\n",
    "\n",
    "__Features__ refer to the __variables that we use to train our model__. Each row of data contains various information that we would like to extract into a training example. Almost all machine learning models ultimately work on numerical representations in the form of a vector; hence, we need to convert raw data into numbers.\n",
    "\n",
    "Features broadly fall into a few categories, which are as follows:\n",
    "\n",
    "__Numerical features:__ These features are typically real or integer numbers, for example, the user age that we used in an example earlier.\n",
    "\n",
    "__Categorical features:__ These features refer to variables that can take one of a set of possible states at any given time. Examples from our dataset might include a user's gender or occupation or movie categories.\n",
    "\n",
    "__Text features:__ These are features derived from the text content in the data, for example, movie titles, descriptions, or reviews.\n",
    "\n",
    "__Other features:__ Most other types of features are ultimately represented numerically. For example, images, video, and audio can be represented as sets of numerical data. Geographical locations can be represented as latitude and longitude or geohash data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then assign index values to each possible occupation in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\n",
    "all_occupations.sort()\n",
    "# create a new dictionary to hold the occupations, and assign the \"1-of-k\" indexes\n",
    "idx = 0\n",
    "all_occupations_dict = {}\n",
    "for o in all_occupations:\n",
    "    all_occupations_dict[o] = idx\n",
    "    idx +=1\n",
    "# try a few examples to see what \"1-of-k\" encoding is assigned\n",
    "print (\"Encoding of 'doctor': %d\" % all_occupations_dict['doctor'])\n",
    "print (\"Encoding of 'programmer': %d\" % all_occupations_dict['programmer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can encode the value of programmer. We will start by creating a numpy array of a length that is equal to the number of possible occupations (k in this case) and filling it with zeros. We will use the zeros function of numpy to create this array.\n",
    "\n",
    "We will then extract the index of the word programmer and assign a value of 1 to the array value at this index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector representation for \"programmer\" and encode it into a binary vector\n",
    "K = len(all_occupations_dict)\n",
    "binary_x = np.zeros(K)\n",
    "k_programmer = all_occupations_dict['programmer']\n",
    "binary_x[k_programmer] = 1\n",
    "#This will give us the resulting binary feature vector of length 21:\n",
    "print (\"Binary feature vector: %s\" % binary_x)\n",
    "print (\"Length of binary vector: %d\" % K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Timestamps into Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to derive categorical features from numerical data, we will use the times of the ratings given by users to movies. These are in the form of Unix timestamps. We can use Python's datetime module to extract the date and time from the timestamp and, in turn, extract the hour of the day. This will result in an RDD of the hour of the day for each rating.\n",
    "\n",
    "We will need a function to extract a datetime representation of the rating timestamp (in seconds); we will create this function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to extract the timestamps (in seconds) from the dataset\n",
    "def extract_datetime(ts):\n",
    "    import datetime\n",
    "    return datetime.datetime.fromtimestamp(ts)\n",
    "# First, we will use a map transformation to extract the timestamp field, \n",
    "# converting it to a Python int datatype. \n",
    "# We will then apply our extract_datetime function to each \n",
    "# timestamp and extract the hour from the resulting datetime object:    \n",
    "timestamps = rating_data.map(lambda fields: int(fields[3]))\n",
    "hour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour)\n",
    "hour_of_day.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for assigning \"time-of-day\" bucket given an hour of the day\n",
    "def assign_tod(hr):\n",
    "    times_of_day = {\n",
    "                'morning' : range(7, 12),\n",
    "                'lunch' : range(12, 14),\n",
    "                'afternoon' : range(14, 18),\n",
    "                'evening' : range(18, 23),\n",
    "                'night' : range(23, 7)\n",
    "                }\n",
    "    for k, v in times_of_day.items():\n",
    "        if hr in v: \n",
    "            return k\n",
    "\n",
    "# now apply the \"time of day\" function to the \"hour of day\" RDD\n",
    "time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\n",
    "time_of_day.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now transformed the timestamp variable (which can take on thousands of values and is probably not useful to a model in its raw form) into hours (taking on 24 values) and then into a time of day (taking on five possible values). Now that we have a categorical feature, we can use the same 1-of-k encoding method outlined earlier to generate a binary feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Text Feature Extraction\n",
    "\n",
    "There are numerous ways of dealing with text, and the field of natural language processing is dedicated to processing, representing, and modeling textual content. A full treatment is beyond the scope of this book, but we will introduce a simple and standard approach for text-feature extraction; this approach is known as the bag-of-words representation.\n",
    "\n",
    "The bag-of-words approach treats a piece of text content as a set of the words, and possibly numbers, in the text (these are often referred to as terms). The process of the bag-of-words approach is as follows:\n",
    "\n",
    "__Tokenization:__ First, some form of tokenization is applied to the text to split it into a set of tokens (generally words, numbers, and so on). An example of this is simple whitespace tokenization, which splits the text on each space and might remove punctuation and other characters that are not alphabetical or numerical.\n",
    "\n",
    "__Stop word removal:__ Next, it is usual to remove very common words such as \"the\", \"and\", and \"but\" (these are known as stop words).\n",
    "\n",
    "__Stemming:__ The next step can include stemming, which refers to taking a term and reducing it to its base form or stem. A common example is plural terms becoming singular (for example, dogs becomes dog and so on). There are many approaches to stemming, and text-processing libraries often contain various stemming algorithms.\n",
    "\n",
    "__Vectorization:__ The final step is turning the processed terms into a vector representation. The simplest form is, perhaps, a binary vector representation, where we assign a value of one if a term exists in the text and zero if it does not. This is essentially identical to the categorical 1-of-k encoding we encountered earlier. Like 1-of-k encoding, this requires a dictionary of terms mapping a given term to an index number. As you might gather, there are potentially millions of individual possible terms (even after stop word removal and stemming). Hence, it becomes critical to use a sparse vector representation where only the fact that a term is present is stored, to save memory and disk space as well as compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function to extract just the title from the raw movie title, \n",
    "# removing the year of release\n",
    "def extract_title(raw):\n",
    "    import re\n",
    "    grps = re.search(\"\\((\\w+)\\)\", raw)    # this regular expression finds the non-word (numbers) between parentheses\n",
    "    if grps:\n",
    "        return raw[:grps.start()].strip() # we strip the trailing whitespace from the title\n",
    "    else:\n",
    "        return raw\n",
    "\n",
    "# first lets extract the raw movie titles from the movie fields\n",
    "raw_titles = movie_fields.map(lambda fields: fields[1])\n",
    "# next, we strip away the \"year of release\" to leave us with just the title text\n",
    "# let's test our title extraction function on the first 5 titles\n",
    "for raw_title in raw_titles.take(5):\n",
    "    print (extract_title(raw_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok that looks good! let's apply it to all the titles\n",
    "movie_titles = raw_titles.map(lambda m: extract_title(m))\n",
    "# next we tokenize the titles into terms. We'll use simple whitespace tokenization\n",
    "title_terms = movie_titles.map(lambda t: t.split(\" \"))\n",
    "print (title_terms.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we would like to collect all the possible terms, in order to build out dictionary of term <-> index mappings\n",
    "all_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n",
    "# create a new dictionary to hold the terms, and assign the \"1-of-k\" indexes\n",
    "idx = 0\n",
    "all_terms_dict = {}\n",
    "for term in all_terms:\n",
    "    all_terms_dict[term] = idx\n",
    "    idx +=1\n",
    "num_terms = len(all_terms_dict)\n",
    "print (\"Total number of terms: %d\" % num_terms)\n",
    "print (\"Index of term 'Dead': %d\" % all_terms_dict['Dead'])\n",
    "print (\"Index of term 'Rooms': %d\" % all_terms_dict['Rooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also use Spark's 'zipWithIndex' RDD function to create the term dictionary\n",
    "all_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "print (\"Index of term 'Dead': %d\" % all_terms_dict2['Dead'])\n",
    "print (\"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a list of terms and encodes it as a scipy sparse vector using an approach \n",
    "# similar to the 1-of-k encoding\n",
    "def create_vector(terms, term_dict):\n",
    "    from scipy import sparse as sp\n",
    "    x = sp.csc_matrix((1, num_terms))\n",
    "    for t in terms:\n",
    "        if t in term_dict:\n",
    "            idx = term_dict[t]\n",
    "            x[0, idx] = 1\n",
    "    return x\n",
    "all_terms_bcast = sc.broadcast(all_terms_dict)\n",
    "term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\n",
    "term_vectors.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Norm of Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10)\n",
    "norm_x_2 = np.linalg.norm(x)\n",
    "normalized_x = x / norm_x_2\n",
    "print (\"x:\\n%s\" % x)\n",
    "print (\"2-Norm of x: %2.4f\" % norm_x_2)\n",
    "print (\"Normalized x:\\n%s\" % normalized_x)\n",
    "print (\"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scaling the Norm of Vectors with MLlib's Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer\n",
    "normalizer = Normalizer()\n",
    "vector = sc.parallelize([x])\n",
    "normalized_x_mllib = normalizer.transform(vector).first().toArray()\n",
    "\n",
    "print (\"x:\\n%s\" % x)\n",
    "print (\"2-Norm of x: %2.4f\" % norm_x_2)\n",
    "print (\"Normalized x MLlib:\\n%s\" % normalized_x_mllib)\n",
    "print (\"2-Norm of normalized_x_mllib: %2.4f\" % np.linalg.norm(normalized_x_mllib))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
